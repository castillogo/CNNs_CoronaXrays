{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'sci.space']\n",
    "data = fetch_20newsgroups(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bil@okcforum.osrhe.edu (Bill Conner)\n",
      "Subject: Re: Not the Omni!\n",
      "Nntp-Posting-Host: okcforum.osrhe.edu\n",
      "Organization: Okcforum Unix Users Group\n",
      "X-Newsreader: TIN [version 1.1 PL6]\n",
      "Lines: 18\n",
      "\n",
      "Charley Wingate (mangoe@cs.umd.edu) wrote:\n",
      ": \n",
      ": >> Please enlighten me.  How is omnipotence contradictory?\n",
      ": \n",
      ": >By definition, all that can occur in the universe is governed by the rules\n",
      ": >of nature. Thus god cannot break them. Anything that god does must be allowed\n",
      ": >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\n",
      ": >the rules of nature.\n",
      ": \n",
      ": Obviously, an omnipotent god can change the rules.\n",
      "\n",
      "When you say, \"By definition\", what exactly is being defined;\n",
      "certainly not omnipotence. You seem to be saying that the \"rules of\n",
      "nature\" are pre-existant somehow, that they not only define nature but\n",
      "actually cause it. If that's what you mean I'd like to hear your\n",
      "further thoughts on the question.\n",
      "\n",
      "Bill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean Text -remove stop words, special characters, lowercase, lemmatize\n",
    "### Vectorize our text - turn our text into number equivalents - create a dictionary mapping from words to numbers  \n",
    "            ### Use Keras Embedding to make word vectors\n",
    "### Create LSTM model\n",
    "### train and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['data']\n",
    "y=data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_my_text(text):\n",
    "    lemmatized = []\n",
    "    text = text.lower()\n",
    "    tokens = model(text)\n",
    "    for word in tokens:\n",
    "        if not word.is_stop and word.is_alpha:\n",
    "            lemmatized.append(word.lemma_)\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bill',\n",
       " 'conner',\n",
       " 'subject',\n",
       " 'omni',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'organization',\n",
       " 'okcforum',\n",
       " 'unix',\n",
       " 'user',\n",
       " 'group',\n",
       " 'x',\n",
       " 'newsreader',\n",
       " 'tin',\n",
       " 'version',\n",
       " 'line',\n",
       " 'charley',\n",
       " 'wingate',\n",
       " 'write',\n",
       " 'enlighten',\n",
       " 'omnipotence',\n",
       " 'contradictory',\n",
       " 'definition',\n",
       " 'occur',\n",
       " 'universe',\n",
       " 'govern',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'god',\n",
       " 'break',\n",
       " 'god',\n",
       " 'allow',\n",
       " 'rule',\n",
       " 'omnipotence',\n",
       " 'exist',\n",
       " 'contradict',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'obviously',\n",
       " 'omnipotent',\n",
       " 'god',\n",
       " 'change',\n",
       " 'rule',\n",
       " 'definition',\n",
       " 'exactly',\n",
       " 'define',\n",
       " 'certainly',\n",
       " 'omnipotence',\n",
       " 'say',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'pre',\n",
       " 'existant',\n",
       " 'define',\n",
       " 'nature',\n",
       " 'actually',\n",
       " 'cause',\n",
       " 'mean',\n",
       " 'like',\n",
       " 'hear',\n",
       " 'thought',\n",
       " 'question',\n",
       " 'bill']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_my_text(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1073/1073 [01:40<00:00, 10.69it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_X = []\n",
    "\n",
    "for text in tqdm.tqdm(X):\n",
    "    results = clean_my_text(text)\n",
    "    clean_X.append(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(clean_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['']\n",
    "for text in clean_X:\n",
    "    for word in text:\n",
    "        vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'bill',\n",
       " 'conner',\n",
       " 'subject',\n",
       " 'omni',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'organization',\n",
       " 'okcforum',\n",
       " 'unix',\n",
       " 'user',\n",
       " 'group',\n",
       " 'x',\n",
       " 'newsreader',\n",
       " 'tin',\n",
       " 'version',\n",
       " 'line',\n",
       " 'charley',\n",
       " 'wingate',\n",
       " 'write',\n",
       " 'enlighten',\n",
       " 'omnipotence',\n",
       " 'contradictory',\n",
       " 'definition',\n",
       " 'occur',\n",
       " 'universe',\n",
       " 'govern',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'god',\n",
       " 'break',\n",
       " 'god',\n",
       " 'allow',\n",
       " 'rule',\n",
       " 'omnipotence',\n",
       " 'exist',\n",
       " 'contradict',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'obviously',\n",
       " 'omnipotent',\n",
       " 'god',\n",
       " 'change',\n",
       " 'rule',\n",
       " 'definition',\n",
       " 'exactly',\n",
       " 'define',\n",
       " 'certainly',\n",
       " 'omnipotence',\n",
       " 'say',\n",
       " 'rule',\n",
       " 'nature',\n",
       " 'pre',\n",
       " 'existant',\n",
       " 'define',\n",
       " 'nature',\n",
       " 'actually',\n",
       " 'cause',\n",
       " 'mean',\n",
       " 'like',\n",
       " 'hear',\n",
       " 'thought',\n",
       " 'question',\n",
       " 'bill',\n",
       " 'jurriaan',\n",
       " 'wittenberg',\n",
       " 'subject',\n",
       " 'magellan',\n",
       " 'update',\n",
       " 'organization',\n",
       " 'utrecht',\n",
       " 'university',\n",
       " 'dept',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'keyword',\n",
       " 'magellan',\n",
       " 'jpl',\n",
       " 'line',\n",
       " 'ron',\n",
       " 'baalke',\n",
       " 'write',\n",
       " 'forward',\n",
       " 'doug',\n",
       " 'griffith',\n",
       " 'magellan',\n",
       " 'project',\n",
       " 'manager',\n",
       " 'magellan',\n",
       " 'status',\n",
       " 'report',\n",
       " 'april',\n",
       " 'magellan',\n",
       " 'complete',\n",
       " 'orbit',\n",
       " 'venus',\n",
       " 'day',\n",
       " 'end',\n",
       " 'start']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list= list(set(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have some data, but now we need to preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "for i, word in enumerate(vocab_list):\n",
    "    num_to_word[i] = word\n",
    "    word_to_num[vocab_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'exotic': 1,\n",
       " 'airplane': 2,\n",
       " 'center': 3,\n",
       " 'karman': 4,\n",
       " 'illegitimate': 5,\n",
       " 'meng': 6,\n",
       " 'zach': 7,\n",
       " 'tomayko': 8,\n",
       " 'disturb': 9,\n",
       " 'ahem': 10,\n",
       " 'neuron': 11,\n",
       " 'deletia': 12,\n",
       " 'pixel': 13,\n",
       " 'cuff': 14,\n",
       " 'warmly': 15,\n",
       " 'tedious': 16,\n",
       " 'durham': 17,\n",
       " 'renoir': 18,\n",
       " 'vw': 19,\n",
       " 'peopel': 20,\n",
       " 'hominum': 21,\n",
       " 'stansbery': 22,\n",
       " 'microfilm': 23,\n",
       " 'subjectively': 24,\n",
       " 'vast': 25,\n",
       " 'bradford': 26,\n",
       " 'mastercard': 27,\n",
       " 'infiltrate': 28,\n",
       " 'establishment': 29,\n",
       " 'polytheist': 30,\n",
       " 'widget': 31,\n",
       " 'technische': 32,\n",
       " 'partook': 33,\n",
       " 'ringlet': 34,\n",
       " 'specifieke': 35,\n",
       " 'night': 36,\n",
       " 'disagreement': 37,\n",
       " 'fairman': 38,\n",
       " 'administer': 39,\n",
       " 'rebroadcast': 40,\n",
       " 'funeral': 41,\n",
       " 'overweight': 42,\n",
       " 'plane': 43,\n",
       " 'city': 44,\n",
       " 'lecture': 45,\n",
       " 'academic': 46,\n",
       " 'blood': 47,\n",
       " 'announcement': 48,\n",
       " 'marketing': 49,\n",
       " 'presently': 50,\n",
       " 'obesity': 51,\n",
       " 'heavy': 52,\n",
       " 'anchor': 53,\n",
       " 'magnetic': 54,\n",
       " 'installation': 55,\n",
       " 'tube': 56,\n",
       " 'criminal': 57,\n",
       " 'library': 58,\n",
       " 'muddy': 59,\n",
       " 'sofia': 60,\n",
       " 'san': 61,\n",
       " 'nuke': 62,\n",
       " 'insecticide': 63,\n",
       " 'desolate': 64,\n",
       " 'anyways': 65,\n",
       " 'scrap': 66,\n",
       " 'initially': 67,\n",
       " 'lossless': 68,\n",
       " 'secessionist': 69,\n",
       " 'midst': 70,\n",
       " 'qualify': 71,\n",
       " 'punish': 72,\n",
       " 'unarmed': 73,\n",
       " 'mechanical': 74,\n",
       " 'sandvick': 75,\n",
       " 'iron': 76,\n",
       " 'foreach': 77,\n",
       " 'cryptography': 78,\n",
       " 'brooks': 79,\n",
       " 'anser': 80,\n",
       " 'evildoer': 81,\n",
       " 'kieran': 82,\n",
       " 'roughly': 83,\n",
       " 'instruct': 84,\n",
       " 'nie': 85,\n",
       " 'mid': 86,\n",
       " 'internationaler': 87,\n",
       " 'rc': 88,\n",
       " 'granger': 89,\n",
       " 'torture': 90,\n",
       " 'advantage': 91,\n",
       " 'rms': 92,\n",
       " 'manipulate': 93,\n",
       " 'csn': 94,\n",
       " 'edinburgh': 95,\n",
       " 'rebuff': 96,\n",
       " 'amherst': 97,\n",
       " 'rtg': 98,\n",
       " 'fort': 99,\n",
       " 'beef': 100,\n",
       " 'ernie': 101,\n",
       " 'jc': 102,\n",
       " 'coreographe': 103,\n",
       " 'flandern': 104,\n",
       " 'sumitomo': 105,\n",
       " 'bolster': 106,\n",
       " 'sperry': 107,\n",
       " 'paint': 108,\n",
       " 'haze': 109,\n",
       " 'yea': 110,\n",
       " 'year': 111,\n",
       " 'portugese': 112,\n",
       " 'dakota': 113,\n",
       " 'rickel': 114,\n",
       " 'storage': 115,\n",
       " 'attractive': 116,\n",
       " 'stressful': 117,\n",
       " 'superintendent': 118,\n",
       " 'heel': 119,\n",
       " 'carlotto': 120,\n",
       " 'vaporize': 121,\n",
       " 'synopsys': 122,\n",
       " 'tarot': 123,\n",
       " 'moreley': 124,\n",
       " 'dobyns': 125,\n",
       " 'chagne': 126,\n",
       " 'missionary': 127,\n",
       " 'showdown': 128,\n",
       " 'neighbor': 129,\n",
       " 'catholics': 130,\n",
       " 'asteroids': 131,\n",
       " 'clown': 132,\n",
       " 'lifer': 133,\n",
       " 'model': 134,\n",
       " 'tolerant': 135,\n",
       " 'contradict': 136,\n",
       " 'arbitron': 137,\n",
       " 'pathogen': 138,\n",
       " 'decorative': 139,\n",
       " 'vastly': 140,\n",
       " 'cosmonaut': 141,\n",
       " 'visual': 142,\n",
       " 'keos': 143,\n",
       " 'greenbelt': 144,\n",
       " 'reschedule': 145,\n",
       " 'ilmenite': 146,\n",
       " 'aap': 147,\n",
       " 'unsophisticated': 148,\n",
       " 'ungodliness': 149,\n",
       " 'ignorantium': 150,\n",
       " 'plt': 151,\n",
       " 'uncluttered': 152,\n",
       " 'cindy': 153,\n",
       " 'robe': 154,\n",
       " 'mstline': 155,\n",
       " 'rare': 156,\n",
       " 'earthbound': 157,\n",
       " 'pull': 158,\n",
       " 'unfavorable': 159,\n",
       " 'verily': 160,\n",
       " 'islamic': 161,\n",
       " 'oliveira': 162,\n",
       " 'salaam': 163,\n",
       " 'telecommunications': 164,\n",
       " 'ervan': 165,\n",
       " 'shoulda': 166,\n",
       " 'clementine': 167,\n",
       " 'mc': 168,\n",
       " 'grantable': 169,\n",
       " 'confrontation': 170,\n",
       " 'evangelize': 171,\n",
       " 'asume': 172,\n",
       " 'nah': 173,\n",
       " 'disneyland': 174,\n",
       " 'transcendental': 175,\n",
       " 'sticker': 176,\n",
       " 'condom': 177,\n",
       " 'include': 178,\n",
       " 'action': 179,\n",
       " 'mean': 180,\n",
       " 'newsgroup': 181,\n",
       " 'international': 182,\n",
       " 'motivation': 183,\n",
       " 'rid': 184,\n",
       " 'say': 185,\n",
       " 'canyon': 186,\n",
       " 'notebook': 187,\n",
       " 'expectation': 188,\n",
       " 'smoking': 189,\n",
       " 'modern': 190,\n",
       " 'element': 191,\n",
       " 'descendant': 192,\n",
       " 'kampf': 193,\n",
       " 'culprit': 194,\n",
       " 'hierarchical': 195,\n",
       " 'combine': 196,\n",
       " 'gullible': 197,\n",
       " 'pepsi': 198,\n",
       " 'federation': 199,\n",
       " 'unwanted': 200,\n",
       " 'geodesy': 201,\n",
       " 'dutch': 202,\n",
       " 'risky': 203,\n",
       " 'cellsat': 204,\n",
       " 'poljot': 205,\n",
       " 'save': 206,\n",
       " 'kr': 207,\n",
       " 'specualtion': 208,\n",
       " 'recoverable': 209,\n",
       " 'character': 210,\n",
       " 'rectangular': 211,\n",
       " 'signalen': 212,\n",
       " 'stang': 213,\n",
       " 'standard': 214,\n",
       " 'networked': 215,\n",
       " 'dating': 216,\n",
       " 'frustrating': 217,\n",
       " 'vertical': 218,\n",
       " 'luther': 219,\n",
       " 'ambiguous': 220,\n",
       " 'missle': 221,\n",
       " 'pasadena': 222,\n",
       " 'gota': 223,\n",
       " 'realistic': 224,\n",
       " 'puzzle': 225,\n",
       " 'polish': 226,\n",
       " 'equally': 227,\n",
       " 'spectrograph': 228,\n",
       " 'nrl': 229,\n",
       " 'moslem': 230,\n",
       " 'cnae': 231,\n",
       " 'sanction': 232,\n",
       " 'window': 233,\n",
       " 'projection': 234,\n",
       " 'saga': 235,\n",
       " 'pessimist': 236,\n",
       " 'lifelong': 237,\n",
       " 'synchronous': 238,\n",
       " 'semantic': 239,\n",
       " 'bodenteich': 240,\n",
       " 'board': 241,\n",
       " 'annihilation': 242,\n",
       " 'wear': 243,\n",
       " 'damaging': 244,\n",
       " 'hsp': 245,\n",
       " 'jed': 246,\n",
       " 'chamber': 247,\n",
       " 'impression': 248,\n",
       " 'scatter': 249,\n",
       " 'recorporation': 250,\n",
       " 'ax': 251,\n",
       " 'automobile': 252,\n",
       " 'inbreede': 253,\n",
       " 'yahweh': 254,\n",
       " 'withstand': 255,\n",
       " 'waldron': 256,\n",
       " 'toaster': 257,\n",
       " 'south': 258,\n",
       " 'accuse': 259,\n",
       " 'enroute': 260,\n",
       " 'nightime': 261,\n",
       " 'sirtf': 262,\n",
       " 'reflection': 263,\n",
       " 'mcelwane': 264,\n",
       " 'interfere': 265,\n",
       " 'explictly': 266,\n",
       " 'steven': 267,\n",
       " 'aforesaid': 268,\n",
       " 'malcolm': 269,\n",
       " 'minnestoa': 270,\n",
       " 'aout': 271,\n",
       " 'suitably': 272,\n",
       " 'message': 273,\n",
       " 'grumman': 274,\n",
       " 'spank': 275,\n",
       " 'bilge': 276,\n",
       " 'csiro': 277,\n",
       " 'sar': 278,\n",
       " 'grover': 279,\n",
       " 'ottawa': 280,\n",
       " 'mill': 281,\n",
       " 'field': 282,\n",
       " 'contain': 283,\n",
       " 'causally': 284,\n",
       " 'unionist': 285,\n",
       " 'phillip': 286,\n",
       " 'contentious': 287,\n",
       " 'meeting': 288,\n",
       " 'dar': 289,\n",
       " 'srmu': 290,\n",
       " 'boss': 291,\n",
       " 'island': 292,\n",
       " 'manifesto': 293,\n",
       " 'israel': 294,\n",
       " 'copyright': 295,\n",
       " 'mobility': 296,\n",
       " 'dynamic': 297,\n",
       " 'submission': 298,\n",
       " 'governance': 299,\n",
       " 'spend': 300,\n",
       " 'tcs': 301,\n",
       " 'prophet': 302,\n",
       " 'baucaille': 303,\n",
       " 'pff': 304,\n",
       " 'apache': 305,\n",
       " 'summon': 306,\n",
       " 'validity': 307,\n",
       " 'pray': 308,\n",
       " 'deg': 309,\n",
       " 'gibbon': 310,\n",
       " 'destroy': 311,\n",
       " 'historical': 312,\n",
       " 'eminently': 313,\n",
       " 'kon': 314,\n",
       " 'improper': 315,\n",
       " 'churchs': 316,\n",
       " 'hijra': 317,\n",
       " 'investement': 318,\n",
       " 'clause': 319,\n",
       " 'discipline': 320,\n",
       " 'evacuate': 321,\n",
       " 'primate': 322,\n",
       " 'collaboration': 323,\n",
       " 'nebula': 324,\n",
       " 'textual': 325,\n",
       " 'differential': 326,\n",
       " 'subjectivist': 327,\n",
       " 'toru': 328,\n",
       " 'knowledge': 329,\n",
       " 'microthruster': 330,\n",
       " 'recommendation': 331,\n",
       " 'incongruency': 332,\n",
       " 'wise': 333,\n",
       " 'mcdonell': 334,\n",
       " 'manned': 335,\n",
       " 'ordinarily': 336,\n",
       " 'uninformed': 337,\n",
       " 'rightfully': 338,\n",
       " 'cambridge': 339,\n",
       " 'gory': 340,\n",
       " 'sousa': 341,\n",
       " 'taipei': 342,\n",
       " 'abduct': 343,\n",
       " 'spouse': 344,\n",
       " 'fri': 345,\n",
       " 'spectroscopy': 346,\n",
       " 'far': 347,\n",
       " 'mail': 348,\n",
       " 'fft': 349,\n",
       " 'slay': 350,\n",
       " 'issn': 351,\n",
       " 'sourcebook': 352,\n",
       " 'committee': 353,\n",
       " 'aocs': 354,\n",
       " 'leash': 355,\n",
       " 'kimeldorf': 356,\n",
       " 'feudal': 357,\n",
       " 'clout': 358,\n",
       " 'feasability': 359,\n",
       " 'sct': 360,\n",
       " 'directness': 361,\n",
       " 'thorough': 362,\n",
       " 'devastating': 363,\n",
       " 'nike': 364,\n",
       " 'yugoslavia': 365,\n",
       " 'clarify': 366,\n",
       " 'paradijs': 367,\n",
       " 'term': 368,\n",
       " 'videoconferencing': 369,\n",
       " 'srb': 370,\n",
       " 'slaying': 371,\n",
       " 'proposition': 372,\n",
       " 'enrico': 373,\n",
       " 'entirity': 374,\n",
       " 'recant': 375,\n",
       " 'beginner': 376,\n",
       " 'tardell': 377,\n",
       " 'float': 378,\n",
       " 'helin': 379,\n",
       " 'compile': 380,\n",
       " 'ensure': 381,\n",
       " 'congressional': 382,\n",
       " 'rude': 383,\n",
       " 'viking': 384,\n",
       " 'dew': 385,\n",
       " 'imputation': 386,\n",
       " 'pittsburgh': 387,\n",
       " 'libertopican': 388,\n",
       " 'welch': 389,\n",
       " 'resurrection': 390,\n",
       " 'delightful': 391,\n",
       " 'bantam': 392,\n",
       " 'credence': 393,\n",
       " 'fullfille': 394,\n",
       " 'usnail': 395,\n",
       " 'reasonable': 396,\n",
       " 'misidentifie': 397,\n",
       " 'amuture': 398,\n",
       " 'posit': 399,\n",
       " 'hurry': 400,\n",
       " 'randomly': 401,\n",
       " 'gulp': 402,\n",
       " 'ethnic': 403,\n",
       " 'wary': 404,\n",
       " 'landscape': 405,\n",
       " 'insane': 406,\n",
       " 'life': 407,\n",
       " 'infanticide': 408,\n",
       " 'deelneman': 409,\n",
       " 'businesswoman': 410,\n",
       " 'kelso': 411,\n",
       " 'electronics': 412,\n",
       " 'fanatically': 413,\n",
       " 'impractical': 414,\n",
       " 'posthumous': 415,\n",
       " 'reply': 416,\n",
       " 'wichita': 417,\n",
       " 'gentile': 418,\n",
       " 'warrant': 419,\n",
       " 'metaphysics': 420,\n",
       " 'conceivable': 421,\n",
       " 'centauri': 422,\n",
       " 'authentic': 423,\n",
       " 'fig': 424,\n",
       " 'treason': 425,\n",
       " 'marc': 426,\n",
       " 'inclusief': 427,\n",
       " 'anchorperson': 428,\n",
       " 'freely': 429,\n",
       " 'franzen': 430,\n",
       " 'coincidentally': 431,\n",
       " 'season': 432,\n",
       " 'engineering': 433,\n",
       " 'emergence': 434,\n",
       " 'zealous': 435,\n",
       " 'kao': 436,\n",
       " 'semitism': 437,\n",
       " 'empirical': 438,\n",
       " 'insightful': 439,\n",
       " 'persecute': 440,\n",
       " 'believe': 441,\n",
       " 'barne': 442,\n",
       " 'consult': 443,\n",
       " 'pas': 444,\n",
       " 'majestic': 445,\n",
       " 'paterson': 446,\n",
       " 'salman': 447,\n",
       " 'blue': 448,\n",
       " 'fueled': 449,\n",
       " 'disqualification': 450,\n",
       " 'mousetrap': 451,\n",
       " 'covalt': 452,\n",
       " 'valuable': 453,\n",
       " 'hhgg': 454,\n",
       " 'quartz': 455,\n",
       " 'survey': 456,\n",
       " 'michael': 457,\n",
       " 'department': 458,\n",
       " 'table': 459,\n",
       " 'yes': 460,\n",
       " 'clumping': 461,\n",
       " 'acronym': 462,\n",
       " 'damage': 463,\n",
       " 'relevant': 464,\n",
       " 'colorado': 465,\n",
       " 'redirect': 466,\n",
       " 'angeles': 467,\n",
       " 'falseness': 468,\n",
       " 'baez': 469,\n",
       " 'stab': 470,\n",
       " 'rough': 471,\n",
       " 'industrialized': 472,\n",
       " 'retired': 473,\n",
       " 'amnesty': 474,\n",
       " 'broker': 475,\n",
       " 'activist': 476,\n",
       " 'scostep': 477,\n",
       " 'sore': 478,\n",
       " 'seinem': 479,\n",
       " 'buddhist': 480,\n",
       " 'grid': 481,\n",
       " 'microwave': 482,\n",
       " 'principii': 483,\n",
       " 'larsonian': 484,\n",
       " 'blunt': 485,\n",
       " 'memoriam': 486,\n",
       " 'graphics': 487,\n",
       " 'ilk': 488,\n",
       " 'noncommercial': 489,\n",
       " 'abbreviation': 490,\n",
       " 'coincidence': 491,\n",
       " 'bgsu': 492,\n",
       " 'honest': 493,\n",
       " 'nicks': 494,\n",
       " 'tunnel': 495,\n",
       " 'mgds': 496,\n",
       " 'obispo': 497,\n",
       " 'grammar': 498,\n",
       " 'furthermore': 499,\n",
       " 'thereof': 500,\n",
       " 'christmas': 501,\n",
       " 'unpressurized': 502,\n",
       " 'difference': 503,\n",
       " 'ssrm': 504,\n",
       " 'commercially': 505,\n",
       " 'imbalance': 506,\n",
       " 'rachel': 507,\n",
       " 'pihko': 508,\n",
       " 'telkens': 509,\n",
       " 'pst': 510,\n",
       " 'mcdonnell': 511,\n",
       " 'definitely': 512,\n",
       " 'david': 513,\n",
       " 'reinstate': 514,\n",
       " 'respectable': 515,\n",
       " 'monitering': 516,\n",
       " 'ray': 517,\n",
       " 'attach': 518,\n",
       " 'pillage': 519,\n",
       " 'working': 520,\n",
       " 'opt': 521,\n",
       " 'fennelly': 522,\n",
       " 'mouse': 523,\n",
       " 'tect': 524,\n",
       " 'clothing': 525,\n",
       " 'tca': 526,\n",
       " 'factual': 527,\n",
       " 'badge': 528,\n",
       " 'oveo': 529,\n",
       " 'chlorine': 530,\n",
       " 'nicknames': 531,\n",
       " 'misgiving': 532,\n",
       " 'arbor': 533,\n",
       " 'waaaay': 534,\n",
       " 'wick': 535,\n",
       " 'imams': 536,\n",
       " 'salameh': 537,\n",
       " 'grandeur': 538,\n",
       " 'suicide': 539,\n",
       " 'mum': 540,\n",
       " 'retrofit': 541,\n",
       " 'rhetorical': 542,\n",
       " 'barbarian': 543,\n",
       " 'slot': 544,\n",
       " 'shari': 545,\n",
       " 'aktueel': 546,\n",
       " 'decisive': 547,\n",
       " 'lampf': 548,\n",
       " 'shamble': 549,\n",
       " 'clip': 550,\n",
       " 'cherish': 551,\n",
       " 'iterative': 552,\n",
       " 'privileged': 553,\n",
       " 'seawater': 554,\n",
       " 'elaboration': 555,\n",
       " 'welcome': 556,\n",
       " 'jupiter': 557,\n",
       " 'makela': 558,\n",
       " 'jelly': 559,\n",
       " 'atividade': 560,\n",
       " 'ephem': 561,\n",
       " 'shocked': 562,\n",
       " 'chamberlin': 563,\n",
       " 'harden': 564,\n",
       " 'pepperell': 565,\n",
       " 'omb': 566,\n",
       " 'usps': 567,\n",
       " 'inteste': 568,\n",
       " 'cain': 569,\n",
       " 'material': 570,\n",
       " 'awesome': 571,\n",
       " 'amro': 572,\n",
       " 'qed': 573,\n",
       " 'expend': 574,\n",
       " 'sudan': 575,\n",
       " 'b': 576,\n",
       " 'tent': 577,\n",
       " 'scrub': 578,\n",
       " 'spawn': 579,\n",
       " 'flr': 580,\n",
       " 'login': 581,\n",
       " 'elephant': 582,\n",
       " 'shoemaker': 583,\n",
       " 'graciosa': 584,\n",
       " 'uriel': 585,\n",
       " 'microwaved': 586,\n",
       " 'jawbone': 587,\n",
       " 'permanently': 588,\n",
       " 'deprogramme': 589,\n",
       " 'hr': 590,\n",
       " 'bus': 591,\n",
       " 'eugene': 592,\n",
       " 'reradiate': 593,\n",
       " 'king': 594,\n",
       " 'murder': 595,\n",
       " 'scanner': 596,\n",
       " 'tsuda': 597,\n",
       " 'tour': 598,\n",
       " 'october': 599,\n",
       " 'large': 600,\n",
       " 'programmer': 601,\n",
       " 'mahound': 602,\n",
       " 'tniy': 603,\n",
       " 'creation': 604,\n",
       " 'misc': 605,\n",
       " 'inherent': 606,\n",
       " 'purdue': 607,\n",
       " 'bubonic': 608,\n",
       " 'confirm': 609,\n",
       " 'empire': 610,\n",
       " 'burrow': 611,\n",
       " 'glitch': 612,\n",
       " 'editing': 613,\n",
       " 'similiar': 614,\n",
       " 'crossposte': 615,\n",
       " 'improved': 616,\n",
       " 'enzymatic': 617,\n",
       " 'investigate': 618,\n",
       " 'quart': 619,\n",
       " 'fly': 620,\n",
       " 'tempe': 621,\n",
       " 'reposte': 622,\n",
       " 'sufferer': 623,\n",
       " 'december': 624,\n",
       " 'outsider': 625,\n",
       " 'tsiolkvosky': 626,\n",
       " 'immorally': 627,\n",
       " 'csm': 628,\n",
       " 'skylab': 629,\n",
       " 'immeasurably': 630,\n",
       " 'sexy': 631,\n",
       " 'sooner': 632,\n",
       " 'piss': 633,\n",
       " 'grip': 634,\n",
       " 'reaction': 635,\n",
       " 'airspace': 636,\n",
       " 'barstow': 637,\n",
       " 'cryogenic': 638,\n",
       " 'khan': 639,\n",
       " 'madman': 640,\n",
       " 'furrow': 641,\n",
       " 'photomultiplier': 642,\n",
       " 'hplabs': 643,\n",
       " 'bloody': 644,\n",
       " 'vanishing': 645,\n",
       " 'punishable': 646,\n",
       " 'betray': 647,\n",
       " 'depositor': 648,\n",
       " 'maintainer': 649,\n",
       " 'insincere': 650,\n",
       " 'curved': 651,\n",
       " 'kiple': 652,\n",
       " 'microsystems': 653,\n",
       " 'silver': 654,\n",
       " 'moscow': 655,\n",
       " 'staget': 656,\n",
       " 'directly': 657,\n",
       " 'geven': 658,\n",
       " 'poorly': 659,\n",
       " 'photoelectric': 660,\n",
       " 'college': 661,\n",
       " 'separated': 662,\n",
       " 'iv': 663,\n",
       " 'flintstone': 664,\n",
       " 'marietta': 665,\n",
       " 'true': 666,\n",
       " 'iowa': 667,\n",
       " 'explosion': 668,\n",
       " 'scan': 669,\n",
       " 'vital': 670,\n",
       " 'revenue': 671,\n",
       " 'figurative': 672,\n",
       " 'bias': 673,\n",
       " 'disapprove': 674,\n",
       " 'wannabe': 675,\n",
       " 'randomness': 676,\n",
       " 'mi': 677,\n",
       " 'ace': 678,\n",
       " 'cosmodrome': 679,\n",
       " 'devour': 680,\n",
       " 'unpatriotic': 681,\n",
       " 'hurt': 682,\n",
       " 'necessarily': 683,\n",
       " 'ventura': 684,\n",
       " 'useful': 685,\n",
       " 'autocorrelation': 686,\n",
       " 'acro': 687,\n",
       " 'vafb': 688,\n",
       " 'mostest': 689,\n",
       " 'perihelion': 690,\n",
       " 'holloway': 691,\n",
       " 'cast': 692,\n",
       " 'lunisolar': 693,\n",
       " 'hurdle': 694,\n",
       " 'unprepared': 695,\n",
       " 'tcp': 696,\n",
       " 'fernando': 697,\n",
       " 'cynthia': 698,\n",
       " 'essay': 699,\n",
       " 'chabrow': 700,\n",
       " 'annually': 701,\n",
       " 'manasseh': 702,\n",
       " 'qumram': 703,\n",
       " 'suopanki': 704,\n",
       " 'canteloupe': 705,\n",
       " 'massacre': 706,\n",
       " 'protestant': 707,\n",
       " 'correspondence': 708,\n",
       " 'flinn': 709,\n",
       " 'cruz': 710,\n",
       " 'screening': 711,\n",
       " 'fabric': 712,\n",
       " 'couple': 713,\n",
       " 'establish': 714,\n",
       " 'axe': 715,\n",
       " 'blackshear': 716,\n",
       " 'tightly': 717,\n",
       " 'running': 718,\n",
       " 'germans': 719,\n",
       " 'useragent': 720,\n",
       " 'rapidly': 721,\n",
       " 'madison': 722,\n",
       " 'jungle': 723,\n",
       " 'finger': 724,\n",
       " 'lmc': 725,\n",
       " 'brightness': 726,\n",
       " 'powell': 727,\n",
       " 'llnl': 728,\n",
       " 'anne': 729,\n",
       " 'lore': 730,\n",
       " 'landsat': 731,\n",
       " 'aurora': 732,\n",
       " 'undertake': 733,\n",
       " 'utilitarian': 734,\n",
       " 'earliest': 735,\n",
       " 'shirt': 736,\n",
       " 'veteran': 737,\n",
       " 'collins': 738,\n",
       " 'gates': 739,\n",
       " 'intergraph': 740,\n",
       " 'questioner': 741,\n",
       " 'deter': 742,\n",
       " 'harmless': 743,\n",
       " 'architecture': 744,\n",
       " 'meaningful': 745,\n",
       " 'coli': 746,\n",
       " 'send': 747,\n",
       " 'fairfax': 748,\n",
       " 'seasnet': 749,\n",
       " 'mainstreame': 750,\n",
       " 'briefing': 751,\n",
       " 'gallileo': 752,\n",
       " 'quiet': 753,\n",
       " 'tome': 754,\n",
       " 'headpiece': 755,\n",
       " 'strategy': 756,\n",
       " 'arrl': 757,\n",
       " 'concert': 758,\n",
       " 'pleading': 759,\n",
       " 'arguer': 760,\n",
       " 'tether': 761,\n",
       " 'nist': 762,\n",
       " 'rate': 763,\n",
       " 'fixture': 764,\n",
       " 'lawful': 765,\n",
       " 'threaten': 766,\n",
       " 'martyriologia': 767,\n",
       " 'secular': 768,\n",
       " 'fundie': 769,\n",
       " 'henling': 770,\n",
       " 'sustained': 771,\n",
       " 'double': 772,\n",
       " 'fairly': 773,\n",
       " 'ronnie': 774,\n",
       " 'charlie': 775,\n",
       " 'cos': 776,\n",
       " 'comfortable': 777,\n",
       " 'allah': 778,\n",
       " 'minneapolis': 779,\n",
       " 'uproarious': 780,\n",
       " 'fittingly': 781,\n",
       " 'acceleration': 782,\n",
       " 'ssme': 783,\n",
       " 'user': 784,\n",
       " 'caliber': 785,\n",
       " 'assault': 786,\n",
       " 'specs': 787,\n",
       " 'unstable': 788,\n",
       " 'coincidental': 789,\n",
       " 'maximise': 790,\n",
       " 'predisposition': 791,\n",
       " 'persist': 792,\n",
       " 'disa': 793,\n",
       " 'smoke': 794,\n",
       " 'annoy': 795,\n",
       " 'shallow': 796,\n",
       " 'diclaimer': 797,\n",
       " 'veertien': 798,\n",
       " 'diver': 799,\n",
       " 'ghost': 800,\n",
       " 'geology': 801,\n",
       " 'hoover': 802,\n",
       " 'tektronix': 803,\n",
       " 'sherzer': 804,\n",
       " 'scripture': 805,\n",
       " 'bub': 806,\n",
       " 'trench': 807,\n",
       " 'ec': 808,\n",
       " 'huang': 809,\n",
       " 'hideously': 810,\n",
       " 'infer': 811,\n",
       " 'forgiving': 812,\n",
       " 'thier': 813,\n",
       " 'accomplishment': 814,\n",
       " 'necessitate': 815,\n",
       " 'assuption': 816,\n",
       " 'deviation': 817,\n",
       " 'hhhmmmmm': 818,\n",
       " 'outta': 819,\n",
       " 'constitute': 820,\n",
       " 'g': 821,\n",
       " 'southeast': 822,\n",
       " 'duty': 823,\n",
       " 'anthony': 824,\n",
       " 'commutative': 825,\n",
       " 'inexact': 826,\n",
       " 'theatre': 827,\n",
       " 'greeley': 828,\n",
       " 'obmb': 829,\n",
       " 'commitment': 830,\n",
       " 'ammos': 831,\n",
       " 'bertrand': 832,\n",
       " 'bureaucrat': 833,\n",
       " 'murtaza': 834,\n",
       " 'ra': 835,\n",
       " 'zeus': 836,\n",
       " 'wais': 837,\n",
       " 'barium': 838,\n",
       " 'rub': 839,\n",
       " 'wrangle': 840,\n",
       " 'gds': 841,\n",
       " 'emasculation': 842,\n",
       " 'silence': 843,\n",
       " 'fuller': 844,\n",
       " 'preemptive': 845,\n",
       " 'kennedy': 846,\n",
       " 'gregg': 847,\n",
       " 'manson': 848,\n",
       " 'egotistical': 849,\n",
       " 'uni': 850,\n",
       " 'avweek': 851,\n",
       " 'loosely': 852,\n",
       " 'vest': 853,\n",
       " 'framework': 854,\n",
       " 'hillary': 855,\n",
       " 'detract': 856,\n",
       " 'slar': 857,\n",
       " 'thenet': 858,\n",
       " 'sized': 859,\n",
       " 'safe': 860,\n",
       " 'shield': 861,\n",
       " 'salt': 862,\n",
       " 'fundy': 863,\n",
       " 'hayashida': 864,\n",
       " 'philosophy': 865,\n",
       " 'spontaneously': 866,\n",
       " 'decrease': 867,\n",
       " 'vue': 868,\n",
       " 'moss': 869,\n",
       " 'archaeology': 870,\n",
       " 'exile': 871,\n",
       " 'bomb': 872,\n",
       " 'bemoan': 873,\n",
       " 'concentrate': 874,\n",
       " 'comic_strip': 875,\n",
       " 'pretention': 876,\n",
       " 'imi': 877,\n",
       " 'soc': 878,\n",
       " 'prediction': 879,\n",
       " 'waarna': 880,\n",
       " 'ground': 881,\n",
       " 'coincide': 882,\n",
       " 'headache': 883,\n",
       " 'smm': 884,\n",
       " 'polite': 885,\n",
       " 'trickle': 886,\n",
       " 'scientist': 887,\n",
       " 'boy': 888,\n",
       " 'leather': 889,\n",
       " 'trachten': 890,\n",
       " 'teh': 891,\n",
       " 'appalachian': 892,\n",
       " 'noticeably': 893,\n",
       " 'tirelessly': 894,\n",
       " 'declination': 895,\n",
       " 'legend': 896,\n",
       " 'czechoslavkia': 897,\n",
       " 'cd': 898,\n",
       " 'suspect': 899,\n",
       " 'el': 900,\n",
       " 'throwing': 901,\n",
       " 'pine': 902,\n",
       " 'sst': 903,\n",
       " 'gastronomic': 904,\n",
       " 'shackle': 905,\n",
       " 'underwriter': 906,\n",
       " 'amicable': 907,\n",
       " 'transformation': 908,\n",
       " 'permannet': 909,\n",
       " 'inthe': 910,\n",
       " 'decide': 911,\n",
       " 'fgs': 912,\n",
       " 'unum': 913,\n",
       " 'islands': 914,\n",
       " 'dense': 915,\n",
       " 'description': 916,\n",
       " 'isu': 917,\n",
       " 'intentional': 918,\n",
       " 'proxima': 919,\n",
       " 'ireland': 920,\n",
       " 'questioning': 921,\n",
       " 'distribute': 922,\n",
       " 'doubtless': 923,\n",
       " 'posy': 924,\n",
       " 'unliked': 925,\n",
       " 'werhner': 926,\n",
       " 'cont': 927,\n",
       " 'pierce': 928,\n",
       " 'gao': 929,\n",
       " 'curious': 930,\n",
       " 'glavcosmos': 931,\n",
       " 'kegs': 932,\n",
       " 'afb': 933,\n",
       " 'dogma': 934,\n",
       " 'revisionist': 935,\n",
       " 'midwesterner': 936,\n",
       " 'messiahship': 937,\n",
       " 'ham': 938,\n",
       " 'daaropvolgende': 939,\n",
       " 'baalke': 940,\n",
       " 'bizzare': 941,\n",
       " 'ispm': 942,\n",
       " 'austin': 943,\n",
       " 'program': 944,\n",
       " 'asuka': 945,\n",
       " 'rcs': 946,\n",
       " 'positions': 947,\n",
       " 'linscott': 948,\n",
       " 'dice': 949,\n",
       " 'uncorrected': 950,\n",
       " 'downey': 951,\n",
       " 'satisfactory': 952,\n",
       " 'passel': 953,\n",
       " 'andsome': 954,\n",
       " 'mysterious': 955,\n",
       " 'metalization': 956,\n",
       " 'court': 957,\n",
       " 'orville': 958,\n",
       " 'plaque': 959,\n",
       " 'wimp': 960,\n",
       " 'uh': 961,\n",
       " 'sara': 962,\n",
       " 'francisco': 963,\n",
       " 'wipp': 964,\n",
       " 'homeland': 965,\n",
       " 'disrupt': 966,\n",
       " 'casually': 967,\n",
       " 'computation': 968,\n",
       " 'oldword': 969,\n",
       " 'pizza': 970,\n",
       " 'difficulty': 971,\n",
       " 'hop': 972,\n",
       " 'purpose': 973,\n",
       " 'darwinist': 974,\n",
       " 'manipulation': 975,\n",
       " 'verify': 976,\n",
       " 'reason': 977,\n",
       " 'populum': 978,\n",
       " 'precedence': 979,\n",
       " 'priest': 980,\n",
       " 'singleton': 981,\n",
       " 'hauck': 982,\n",
       " 'vile': 983,\n",
       " 'jihad': 984,\n",
       " 'deserve': 985,\n",
       " 'tone': 986,\n",
       " 'ascii': 987,\n",
       " 'swindle': 988,\n",
       " 'foul': 989,\n",
       " 'erect': 990,\n",
       " 'score': 991,\n",
       " 'query': 992,\n",
       " 'dilute': 993,\n",
       " 'georgy': 994,\n",
       " 'perk': 995,\n",
       " 'est': 996,\n",
       " 'mariatta': 997,\n",
       " 'sheaffer': 998,\n",
       " 'extradinary': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: 'exotic',\n",
       " 2: 'airplane',\n",
       " 3: 'center',\n",
       " 4: 'karman',\n",
       " 5: 'illegitimate',\n",
       " 6: 'meng',\n",
       " 7: 'zach',\n",
       " 8: 'tomayko',\n",
       " 9: 'disturb',\n",
       " 10: 'ahem',\n",
       " 11: 'neuron',\n",
       " 12: 'deletia',\n",
       " 13: 'pixel',\n",
       " 14: 'cuff',\n",
       " 15: 'warmly',\n",
       " 16: 'tedious',\n",
       " 17: 'durham',\n",
       " 18: 'renoir',\n",
       " 19: 'vw',\n",
       " 20: 'peopel',\n",
       " 21: 'hominum',\n",
       " 22: 'stansbery',\n",
       " 23: 'microfilm',\n",
       " 24: 'subjectively',\n",
       " 25: 'vast',\n",
       " 26: 'bradford',\n",
       " 27: 'mastercard',\n",
       " 28: 'infiltrate',\n",
       " 29: 'establishment',\n",
       " 30: 'polytheist',\n",
       " 31: 'widget',\n",
       " 32: 'technische',\n",
       " 33: 'partook',\n",
       " 34: 'ringlet',\n",
       " 35: 'specifieke',\n",
       " 36: 'night',\n",
       " 37: 'disagreement',\n",
       " 38: 'fairman',\n",
       " 39: 'administer',\n",
       " 40: 'rebroadcast',\n",
       " 41: 'funeral',\n",
       " 42: 'overweight',\n",
       " 43: 'plane',\n",
       " 44: 'city',\n",
       " 45: 'lecture',\n",
       " 46: 'academic',\n",
       " 47: 'blood',\n",
       " 48: 'announcement',\n",
       " 49: 'marketing',\n",
       " 50: 'presently',\n",
       " 51: 'obesity',\n",
       " 52: 'heavy',\n",
       " 53: 'anchor',\n",
       " 54: 'magnetic',\n",
       " 55: 'installation',\n",
       " 56: 'tube',\n",
       " 57: 'criminal',\n",
       " 58: 'library',\n",
       " 59: 'muddy',\n",
       " 60: 'sofia',\n",
       " 61: 'san',\n",
       " 62: 'nuke',\n",
       " 63: 'insecticide',\n",
       " 64: 'desolate',\n",
       " 65: 'anyways',\n",
       " 66: 'scrap',\n",
       " 67: 'initially',\n",
       " 68: 'lossless',\n",
       " 69: 'secessionist',\n",
       " 70: 'midst',\n",
       " 71: 'qualify',\n",
       " 72: 'punish',\n",
       " 73: 'unarmed',\n",
       " 74: 'mechanical',\n",
       " 75: 'sandvick',\n",
       " 76: 'iron',\n",
       " 77: 'foreach',\n",
       " 78: 'cryptography',\n",
       " 79: 'brooks',\n",
       " 80: 'anser',\n",
       " 81: 'evildoer',\n",
       " 82: 'kieran',\n",
       " 83: 'roughly',\n",
       " 84: 'instruct',\n",
       " 85: 'nie',\n",
       " 86: 'mid',\n",
       " 87: 'internationaler',\n",
       " 88: 'rc',\n",
       " 89: 'granger',\n",
       " 90: 'torture',\n",
       " 91: 'advantage',\n",
       " 92: 'rms',\n",
       " 93: 'manipulate',\n",
       " 94: 'csn',\n",
       " 95: 'edinburgh',\n",
       " 96: 'rebuff',\n",
       " 97: 'amherst',\n",
       " 98: 'rtg',\n",
       " 99: 'fort',\n",
       " 100: 'beef',\n",
       " 101: 'ernie',\n",
       " 102: 'jc',\n",
       " 103: 'coreographe',\n",
       " 104: 'flandern',\n",
       " 105: 'sumitomo',\n",
       " 106: 'bolster',\n",
       " 107: 'sperry',\n",
       " 108: 'paint',\n",
       " 109: 'haze',\n",
       " 110: 'yea',\n",
       " 111: 'year',\n",
       " 112: 'portugese',\n",
       " 113: 'dakota',\n",
       " 114: 'rickel',\n",
       " 115: 'storage',\n",
       " 116: 'attractive',\n",
       " 117: 'stressful',\n",
       " 118: 'superintendent',\n",
       " 119: 'heel',\n",
       " 120: 'carlotto',\n",
       " 121: 'vaporize',\n",
       " 122: 'synopsys',\n",
       " 123: 'tarot',\n",
       " 124: 'moreley',\n",
       " 125: 'dobyns',\n",
       " 126: 'chagne',\n",
       " 127: 'missionary',\n",
       " 128: 'showdown',\n",
       " 129: 'neighbor',\n",
       " 130: 'catholics',\n",
       " 131: 'asteroids',\n",
       " 132: 'clown',\n",
       " 133: 'lifer',\n",
       " 134: 'model',\n",
       " 135: 'tolerant',\n",
       " 136: 'contradict',\n",
       " 137: 'arbitron',\n",
       " 138: 'pathogen',\n",
       " 139: 'decorative',\n",
       " 140: 'vastly',\n",
       " 141: 'cosmonaut',\n",
       " 142: 'visual',\n",
       " 143: 'keos',\n",
       " 144: 'greenbelt',\n",
       " 145: 'reschedule',\n",
       " 146: 'ilmenite',\n",
       " 147: 'aap',\n",
       " 148: 'unsophisticated',\n",
       " 149: 'ungodliness',\n",
       " 150: 'ignorantium',\n",
       " 151: 'plt',\n",
       " 152: 'uncluttered',\n",
       " 153: 'cindy',\n",
       " 154: 'robe',\n",
       " 155: 'mstline',\n",
       " 156: 'rare',\n",
       " 157: 'earthbound',\n",
       " 158: 'pull',\n",
       " 159: 'unfavorable',\n",
       " 160: 'verily',\n",
       " 161: 'islamic',\n",
       " 162: 'oliveira',\n",
       " 163: 'salaam',\n",
       " 164: 'telecommunications',\n",
       " 165: 'ervan',\n",
       " 166: 'shoulda',\n",
       " 167: 'clementine',\n",
       " 168: 'mc',\n",
       " 169: 'grantable',\n",
       " 170: 'confrontation',\n",
       " 171: 'evangelize',\n",
       " 172: 'asume',\n",
       " 173: 'nah',\n",
       " 174: 'disneyland',\n",
       " 175: 'transcendental',\n",
       " 176: 'sticker',\n",
       " 177: 'condom',\n",
       " 178: 'include',\n",
       " 179: 'action',\n",
       " 180: 'mean',\n",
       " 181: 'newsgroup',\n",
       " 182: 'international',\n",
       " 183: 'motivation',\n",
       " 184: 'rid',\n",
       " 185: 'say',\n",
       " 186: 'canyon',\n",
       " 187: 'notebook',\n",
       " 188: 'expectation',\n",
       " 189: 'smoking',\n",
       " 190: 'modern',\n",
       " 191: 'element',\n",
       " 192: 'descendant',\n",
       " 193: 'kampf',\n",
       " 194: 'culprit',\n",
       " 195: 'hierarchical',\n",
       " 196: 'combine',\n",
       " 197: 'gullible',\n",
       " 198: 'pepsi',\n",
       " 199: 'federation',\n",
       " 200: 'unwanted',\n",
       " 201: 'geodesy',\n",
       " 202: 'dutch',\n",
       " 203: 'risky',\n",
       " 204: 'cellsat',\n",
       " 205: 'poljot',\n",
       " 206: 'save',\n",
       " 207: 'kr',\n",
       " 208: 'specualtion',\n",
       " 209: 'recoverable',\n",
       " 210: 'character',\n",
       " 211: 'rectangular',\n",
       " 212: 'signalen',\n",
       " 213: 'stang',\n",
       " 214: 'standard',\n",
       " 215: 'networked',\n",
       " 216: 'dating',\n",
       " 217: 'frustrating',\n",
       " 218: 'vertical',\n",
       " 219: 'luther',\n",
       " 220: 'ambiguous',\n",
       " 221: 'missle',\n",
       " 222: 'pasadena',\n",
       " 223: 'gota',\n",
       " 224: 'realistic',\n",
       " 225: 'puzzle',\n",
       " 226: 'polish',\n",
       " 227: 'equally',\n",
       " 228: 'spectrograph',\n",
       " 229: 'nrl',\n",
       " 230: 'moslem',\n",
       " 231: 'cnae',\n",
       " 232: 'sanction',\n",
       " 233: 'window',\n",
       " 234: 'projection',\n",
       " 235: 'saga',\n",
       " 236: 'pessimist',\n",
       " 237: 'lifelong',\n",
       " 238: 'synchronous',\n",
       " 239: 'semantic',\n",
       " 240: 'bodenteich',\n",
       " 241: 'board',\n",
       " 242: 'annihilation',\n",
       " 243: 'wear',\n",
       " 244: 'damaging',\n",
       " 245: 'hsp',\n",
       " 246: 'jed',\n",
       " 247: 'chamber',\n",
       " 248: 'impression',\n",
       " 249: 'scatter',\n",
       " 250: 'recorporation',\n",
       " 251: 'ax',\n",
       " 252: 'automobile',\n",
       " 253: 'inbreede',\n",
       " 254: 'yahweh',\n",
       " 255: 'withstand',\n",
       " 256: 'waldron',\n",
       " 257: 'toaster',\n",
       " 258: 'south',\n",
       " 259: 'accuse',\n",
       " 260: 'enroute',\n",
       " 261: 'nightime',\n",
       " 262: 'sirtf',\n",
       " 263: 'reflection',\n",
       " 264: 'mcelwane',\n",
       " 265: 'interfere',\n",
       " 266: 'explictly',\n",
       " 267: 'steven',\n",
       " 268: 'aforesaid',\n",
       " 269: 'malcolm',\n",
       " 270: 'minnestoa',\n",
       " 271: 'aout',\n",
       " 272: 'suitably',\n",
       " 273: 'message',\n",
       " 274: 'grumman',\n",
       " 275: 'spank',\n",
       " 276: 'bilge',\n",
       " 277: 'csiro',\n",
       " 278: 'sar',\n",
       " 279: 'grover',\n",
       " 280: 'ottawa',\n",
       " 281: 'mill',\n",
       " 282: 'field',\n",
       " 283: 'contain',\n",
       " 284: 'causally',\n",
       " 285: 'unionist',\n",
       " 286: 'phillip',\n",
       " 287: 'contentious',\n",
       " 288: 'meeting',\n",
       " 289: 'dar',\n",
       " 290: 'srmu',\n",
       " 291: 'boss',\n",
       " 292: 'island',\n",
       " 293: 'manifesto',\n",
       " 294: 'israel',\n",
       " 295: 'copyright',\n",
       " 296: 'mobility',\n",
       " 297: 'dynamic',\n",
       " 298: 'submission',\n",
       " 299: 'governance',\n",
       " 300: 'spend',\n",
       " 301: 'tcs',\n",
       " 302: 'prophet',\n",
       " 303: 'baucaille',\n",
       " 304: 'pff',\n",
       " 305: 'apache',\n",
       " 306: 'summon',\n",
       " 307: 'validity',\n",
       " 308: 'pray',\n",
       " 309: 'deg',\n",
       " 310: 'gibbon',\n",
       " 311: 'destroy',\n",
       " 312: 'historical',\n",
       " 313: 'eminently',\n",
       " 314: 'kon',\n",
       " 315: 'improper',\n",
       " 316: 'churchs',\n",
       " 317: 'hijra',\n",
       " 318: 'investement',\n",
       " 319: 'clause',\n",
       " 320: 'discipline',\n",
       " 321: 'evacuate',\n",
       " 322: 'primate',\n",
       " 323: 'collaboration',\n",
       " 324: 'nebula',\n",
       " 325: 'textual',\n",
       " 326: 'differential',\n",
       " 327: 'subjectivist',\n",
       " 328: 'toru',\n",
       " 329: 'knowledge',\n",
       " 330: 'microthruster',\n",
       " 331: 'recommendation',\n",
       " 332: 'incongruency',\n",
       " 333: 'wise',\n",
       " 334: 'mcdonell',\n",
       " 335: 'manned',\n",
       " 336: 'ordinarily',\n",
       " 337: 'uninformed',\n",
       " 338: 'rightfully',\n",
       " 339: 'cambridge',\n",
       " 340: 'gory',\n",
       " 341: 'sousa',\n",
       " 342: 'taipei',\n",
       " 343: 'abduct',\n",
       " 344: 'spouse',\n",
       " 345: 'fri',\n",
       " 346: 'spectroscopy',\n",
       " 347: 'far',\n",
       " 348: 'mail',\n",
       " 349: 'fft',\n",
       " 350: 'slay',\n",
       " 351: 'issn',\n",
       " 352: 'sourcebook',\n",
       " 353: 'committee',\n",
       " 354: 'aocs',\n",
       " 355: 'leash',\n",
       " 356: 'kimeldorf',\n",
       " 357: 'feudal',\n",
       " 358: 'clout',\n",
       " 359: 'feasability',\n",
       " 360: 'sct',\n",
       " 361: 'directness',\n",
       " 362: 'thorough',\n",
       " 363: 'devastating',\n",
       " 364: 'nike',\n",
       " 365: 'yugoslavia',\n",
       " 366: 'clarify',\n",
       " 367: 'paradijs',\n",
       " 368: 'term',\n",
       " 369: 'videoconferencing',\n",
       " 370: 'srb',\n",
       " 371: 'slaying',\n",
       " 372: 'proposition',\n",
       " 373: 'enrico',\n",
       " 374: 'entirity',\n",
       " 375: 'recant',\n",
       " 376: 'beginner',\n",
       " 377: 'tardell',\n",
       " 378: 'float',\n",
       " 379: 'helin',\n",
       " 380: 'compile',\n",
       " 381: 'ensure',\n",
       " 382: 'congressional',\n",
       " 383: 'rude',\n",
       " 384: 'viking',\n",
       " 385: 'dew',\n",
       " 386: 'imputation',\n",
       " 387: 'pittsburgh',\n",
       " 388: 'libertopican',\n",
       " 389: 'welch',\n",
       " 390: 'resurrection',\n",
       " 391: 'delightful',\n",
       " 392: 'bantam',\n",
       " 393: 'credence',\n",
       " 394: 'fullfille',\n",
       " 395: 'usnail',\n",
       " 396: 'reasonable',\n",
       " 397: 'misidentifie',\n",
       " 398: 'amuture',\n",
       " 399: 'posit',\n",
       " 400: 'hurry',\n",
       " 401: 'randomly',\n",
       " 402: 'gulp',\n",
       " 403: 'ethnic',\n",
       " 404: 'wary',\n",
       " 405: 'landscape',\n",
       " 406: 'insane',\n",
       " 407: 'life',\n",
       " 408: 'infanticide',\n",
       " 409: 'deelneman',\n",
       " 410: 'businesswoman',\n",
       " 411: 'kelso',\n",
       " 412: 'electronics',\n",
       " 413: 'fanatically',\n",
       " 414: 'impractical',\n",
       " 415: 'posthumous',\n",
       " 416: 'reply',\n",
       " 417: 'wichita',\n",
       " 418: 'gentile',\n",
       " 419: 'warrant',\n",
       " 420: 'metaphysics',\n",
       " 421: 'conceivable',\n",
       " 422: 'centauri',\n",
       " 423: 'authentic',\n",
       " 424: 'fig',\n",
       " 425: 'treason',\n",
       " 426: 'marc',\n",
       " 427: 'inclusief',\n",
       " 428: 'anchorperson',\n",
       " 429: 'freely',\n",
       " 430: 'franzen',\n",
       " 431: 'coincidentally',\n",
       " 432: 'season',\n",
       " 433: 'engineering',\n",
       " 434: 'emergence',\n",
       " 435: 'zealous',\n",
       " 436: 'kao',\n",
       " 437: 'semitism',\n",
       " 438: 'empirical',\n",
       " 439: 'insightful',\n",
       " 440: 'persecute',\n",
       " 441: 'believe',\n",
       " 442: 'barne',\n",
       " 443: 'consult',\n",
       " 444: 'pas',\n",
       " 445: 'majestic',\n",
       " 446: 'paterson',\n",
       " 447: 'salman',\n",
       " 448: 'blue',\n",
       " 449: 'fueled',\n",
       " 450: 'disqualification',\n",
       " 451: 'mousetrap',\n",
       " 452: 'covalt',\n",
       " 453: 'valuable',\n",
       " 454: 'hhgg',\n",
       " 455: 'quartz',\n",
       " 456: 'survey',\n",
       " 457: 'michael',\n",
       " 458: 'department',\n",
       " 459: 'table',\n",
       " 460: 'yes',\n",
       " 461: 'clumping',\n",
       " 462: 'acronym',\n",
       " 463: 'damage',\n",
       " 464: 'relevant',\n",
       " 465: 'colorado',\n",
       " 466: 'redirect',\n",
       " 467: 'angeles',\n",
       " 468: 'falseness',\n",
       " 469: 'baez',\n",
       " 470: 'stab',\n",
       " 471: 'rough',\n",
       " 472: 'industrialized',\n",
       " 473: 'retired',\n",
       " 474: 'amnesty',\n",
       " 475: 'broker',\n",
       " 476: 'activist',\n",
       " 477: 'scostep',\n",
       " 478: 'sore',\n",
       " 479: 'seinem',\n",
       " 480: 'buddhist',\n",
       " 481: 'grid',\n",
       " 482: 'microwave',\n",
       " 483: 'principii',\n",
       " 484: 'larsonian',\n",
       " 485: 'blunt',\n",
       " 486: 'memoriam',\n",
       " 487: 'graphics',\n",
       " 488: 'ilk',\n",
       " 489: 'noncommercial',\n",
       " 490: 'abbreviation',\n",
       " 491: 'coincidence',\n",
       " 492: 'bgsu',\n",
       " 493: 'honest',\n",
       " 494: 'nicks',\n",
       " 495: 'tunnel',\n",
       " 496: 'mgds',\n",
       " 497: 'obispo',\n",
       " 498: 'grammar',\n",
       " 499: 'furthermore',\n",
       " 500: 'thereof',\n",
       " 501: 'christmas',\n",
       " 502: 'unpressurized',\n",
       " 503: 'difference',\n",
       " 504: 'ssrm',\n",
       " 505: 'commercially',\n",
       " 506: 'imbalance',\n",
       " 507: 'rachel',\n",
       " 508: 'pihko',\n",
       " 509: 'telkens',\n",
       " 510: 'pst',\n",
       " 511: 'mcdonnell',\n",
       " 512: 'definitely',\n",
       " 513: 'david',\n",
       " 514: 'reinstate',\n",
       " 515: 'respectable',\n",
       " 516: 'monitering',\n",
       " 517: 'ray',\n",
       " 518: 'attach',\n",
       " 519: 'pillage',\n",
       " 520: 'working',\n",
       " 521: 'opt',\n",
       " 522: 'fennelly',\n",
       " 523: 'mouse',\n",
       " 524: 'tect',\n",
       " 525: 'clothing',\n",
       " 526: 'tca',\n",
       " 527: 'factual',\n",
       " 528: 'badge',\n",
       " 529: 'oveo',\n",
       " 530: 'chlorine',\n",
       " 531: 'nicknames',\n",
       " 532: 'misgiving',\n",
       " 533: 'arbor',\n",
       " 534: 'waaaay',\n",
       " 535: 'wick',\n",
       " 536: 'imams',\n",
       " 537: 'salameh',\n",
       " 538: 'grandeur',\n",
       " 539: 'suicide',\n",
       " 540: 'mum',\n",
       " 541: 'retrofit',\n",
       " 542: 'rhetorical',\n",
       " 543: 'barbarian',\n",
       " 544: 'slot',\n",
       " 545: 'shari',\n",
       " 546: 'aktueel',\n",
       " 547: 'decisive',\n",
       " 548: 'lampf',\n",
       " 549: 'shamble',\n",
       " 550: 'clip',\n",
       " 551: 'cherish',\n",
       " 552: 'iterative',\n",
       " 553: 'privileged',\n",
       " 554: 'seawater',\n",
       " 555: 'elaboration',\n",
       " 556: 'welcome',\n",
       " 557: 'jupiter',\n",
       " 558: 'makela',\n",
       " 559: 'jelly',\n",
       " 560: 'atividade',\n",
       " 561: 'ephem',\n",
       " 562: 'shocked',\n",
       " 563: 'chamberlin',\n",
       " 564: 'harden',\n",
       " 565: 'pepperell',\n",
       " 566: 'omb',\n",
       " 567: 'usps',\n",
       " 568: 'inteste',\n",
       " 569: 'cain',\n",
       " 570: 'material',\n",
       " 571: 'awesome',\n",
       " 572: 'amro',\n",
       " 573: 'qed',\n",
       " 574: 'expend',\n",
       " 575: 'sudan',\n",
       " 576: 'b',\n",
       " 577: 'tent',\n",
       " 578: 'scrub',\n",
       " 579: 'spawn',\n",
       " 580: 'flr',\n",
       " 581: 'login',\n",
       " 582: 'elephant',\n",
       " 583: 'shoemaker',\n",
       " 584: 'graciosa',\n",
       " 585: 'uriel',\n",
       " 586: 'microwaved',\n",
       " 587: 'jawbone',\n",
       " 588: 'permanently',\n",
       " 589: 'deprogramme',\n",
       " 590: 'hr',\n",
       " 591: 'bus',\n",
       " 592: 'eugene',\n",
       " 593: 'reradiate',\n",
       " 594: 'king',\n",
       " 595: 'murder',\n",
       " 596: 'scanner',\n",
       " 597: 'tsuda',\n",
       " 598: 'tour',\n",
       " 599: 'october',\n",
       " 600: 'large',\n",
       " 601: 'programmer',\n",
       " 602: 'mahound',\n",
       " 603: 'tniy',\n",
       " 604: 'creation',\n",
       " 605: 'misc',\n",
       " 606: 'inherent',\n",
       " 607: 'purdue',\n",
       " 608: 'bubonic',\n",
       " 609: 'confirm',\n",
       " 610: 'empire',\n",
       " 611: 'burrow',\n",
       " 612: 'glitch',\n",
       " 613: 'editing',\n",
       " 614: 'similiar',\n",
       " 615: 'crossposte',\n",
       " 616: 'improved',\n",
       " 617: 'enzymatic',\n",
       " 618: 'investigate',\n",
       " 619: 'quart',\n",
       " 620: 'fly',\n",
       " 621: 'tempe',\n",
       " 622: 'reposte',\n",
       " 623: 'sufferer',\n",
       " 624: 'december',\n",
       " 625: 'outsider',\n",
       " 626: 'tsiolkvosky',\n",
       " 627: 'immorally',\n",
       " 628: 'csm',\n",
       " 629: 'skylab',\n",
       " 630: 'immeasurably',\n",
       " 631: 'sexy',\n",
       " 632: 'sooner',\n",
       " 633: 'piss',\n",
       " 634: 'grip',\n",
       " 635: 'reaction',\n",
       " 636: 'airspace',\n",
       " 637: 'barstow',\n",
       " 638: 'cryogenic',\n",
       " 639: 'khan',\n",
       " 640: 'madman',\n",
       " 641: 'furrow',\n",
       " 642: 'photomultiplier',\n",
       " 643: 'hplabs',\n",
       " 644: 'bloody',\n",
       " 645: 'vanishing',\n",
       " 646: 'punishable',\n",
       " 647: 'betray',\n",
       " 648: 'depositor',\n",
       " 649: 'maintainer',\n",
       " 650: 'insincere',\n",
       " 651: 'curved',\n",
       " 652: 'kiple',\n",
       " 653: 'microsystems',\n",
       " 654: 'silver',\n",
       " 655: 'moscow',\n",
       " 656: 'staget',\n",
       " 657: 'directly',\n",
       " 658: 'geven',\n",
       " 659: 'poorly',\n",
       " 660: 'photoelectric',\n",
       " 661: 'college',\n",
       " 662: 'separated',\n",
       " 663: 'iv',\n",
       " 664: 'flintstone',\n",
       " 665: 'marietta',\n",
       " 666: 'true',\n",
       " 667: 'iowa',\n",
       " 668: 'explosion',\n",
       " 669: 'scan',\n",
       " 670: 'vital',\n",
       " 671: 'revenue',\n",
       " 672: 'figurative',\n",
       " 673: 'bias',\n",
       " 674: 'disapprove',\n",
       " 675: 'wannabe',\n",
       " 676: 'randomness',\n",
       " 677: 'mi',\n",
       " 678: 'ace',\n",
       " 679: 'cosmodrome',\n",
       " 680: 'devour',\n",
       " 681: 'unpatriotic',\n",
       " 682: 'hurt',\n",
       " 683: 'necessarily',\n",
       " 684: 'ventura',\n",
       " 685: 'useful',\n",
       " 686: 'autocorrelation',\n",
       " 687: 'acro',\n",
       " 688: 'vafb',\n",
       " 689: 'mostest',\n",
       " 690: 'perihelion',\n",
       " 691: 'holloway',\n",
       " 692: 'cast',\n",
       " 693: 'lunisolar',\n",
       " 694: 'hurdle',\n",
       " 695: 'unprepared',\n",
       " 696: 'tcp',\n",
       " 697: 'fernando',\n",
       " 698: 'cynthia',\n",
       " 699: 'essay',\n",
       " 700: 'chabrow',\n",
       " 701: 'annually',\n",
       " 702: 'manasseh',\n",
       " 703: 'qumram',\n",
       " 704: 'suopanki',\n",
       " 705: 'canteloupe',\n",
       " 706: 'massacre',\n",
       " 707: 'protestant',\n",
       " 708: 'correspondence',\n",
       " 709: 'flinn',\n",
       " 710: 'cruz',\n",
       " 711: 'screening',\n",
       " 712: 'fabric',\n",
       " 713: 'couple',\n",
       " 714: 'establish',\n",
       " 715: 'axe',\n",
       " 716: 'blackshear',\n",
       " 717: 'tightly',\n",
       " 718: 'running',\n",
       " 719: 'germans',\n",
       " 720: 'useragent',\n",
       " 721: 'rapidly',\n",
       " 722: 'madison',\n",
       " 723: 'jungle',\n",
       " 724: 'finger',\n",
       " 725: 'lmc',\n",
       " 726: 'brightness',\n",
       " 727: 'powell',\n",
       " 728: 'llnl',\n",
       " 729: 'anne',\n",
       " 730: 'lore',\n",
       " 731: 'landsat',\n",
       " 732: 'aurora',\n",
       " 733: 'undertake',\n",
       " 734: 'utilitarian',\n",
       " 735: 'earliest',\n",
       " 736: 'shirt',\n",
       " 737: 'veteran',\n",
       " 738: 'collins',\n",
       " 739: 'gates',\n",
       " 740: 'intergraph',\n",
       " 741: 'questioner',\n",
       " 742: 'deter',\n",
       " 743: 'harmless',\n",
       " 744: 'architecture',\n",
       " 745: 'meaningful',\n",
       " 746: 'coli',\n",
       " 747: 'send',\n",
       " 748: 'fairfax',\n",
       " 749: 'seasnet',\n",
       " 750: 'mainstreame',\n",
       " 751: 'briefing',\n",
       " 752: 'gallileo',\n",
       " 753: 'quiet',\n",
       " 754: 'tome',\n",
       " 755: 'headpiece',\n",
       " 756: 'strategy',\n",
       " 757: 'arrl',\n",
       " 758: 'concert',\n",
       " 759: 'pleading',\n",
       " 760: 'arguer',\n",
       " 761: 'tether',\n",
       " 762: 'nist',\n",
       " 763: 'rate',\n",
       " 764: 'fixture',\n",
       " 765: 'lawful',\n",
       " 766: 'threaten',\n",
       " 767: 'martyriologia',\n",
       " 768: 'secular',\n",
       " 769: 'fundie',\n",
       " 770: 'henling',\n",
       " 771: 'sustained',\n",
       " 772: 'double',\n",
       " 773: 'fairly',\n",
       " 774: 'ronnie',\n",
       " 775: 'charlie',\n",
       " 776: 'cos',\n",
       " 777: 'comfortable',\n",
       " 778: 'allah',\n",
       " 779: 'minneapolis',\n",
       " 780: 'uproarious',\n",
       " 781: 'fittingly',\n",
       " 782: 'acceleration',\n",
       " 783: 'ssme',\n",
       " 784: 'user',\n",
       " 785: 'caliber',\n",
       " 786: 'assault',\n",
       " 787: 'specs',\n",
       " 788: 'unstable',\n",
       " 789: 'coincidental',\n",
       " 790: 'maximise',\n",
       " 791: 'predisposition',\n",
       " 792: 'persist',\n",
       " 793: 'disa',\n",
       " 794: 'smoke',\n",
       " 795: 'annoy',\n",
       " 796: 'shallow',\n",
       " 797: 'diclaimer',\n",
       " 798: 'veertien',\n",
       " 799: 'diver',\n",
       " 800: 'ghost',\n",
       " 801: 'geology',\n",
       " 802: 'hoover',\n",
       " 803: 'tektronix',\n",
       " 804: 'sherzer',\n",
       " 805: 'scripture',\n",
       " 806: 'bub',\n",
       " 807: 'trench',\n",
       " 808: 'ec',\n",
       " 809: 'huang',\n",
       " 810: 'hideously',\n",
       " 811: 'infer',\n",
       " 812: 'forgiving',\n",
       " 813: 'thier',\n",
       " 814: 'accomplishment',\n",
       " 815: 'necessitate',\n",
       " 816: 'assuption',\n",
       " 817: 'deviation',\n",
       " 818: 'hhhmmmmm',\n",
       " 819: 'outta',\n",
       " 820: 'constitute',\n",
       " 821: 'g',\n",
       " 822: 'southeast',\n",
       " 823: 'duty',\n",
       " 824: 'anthony',\n",
       " 825: 'commutative',\n",
       " 826: 'inexact',\n",
       " 827: 'theatre',\n",
       " 828: 'greeley',\n",
       " 829: 'obmb',\n",
       " 830: 'commitment',\n",
       " 831: 'ammos',\n",
       " 832: 'bertrand',\n",
       " 833: 'bureaucrat',\n",
       " 834: 'murtaza',\n",
       " 835: 'ra',\n",
       " 836: 'zeus',\n",
       " 837: 'wais',\n",
       " 838: 'barium',\n",
       " 839: 'rub',\n",
       " 840: 'wrangle',\n",
       " 841: 'gds',\n",
       " 842: 'emasculation',\n",
       " 843: 'silence',\n",
       " 844: 'fuller',\n",
       " 845: 'preemptive',\n",
       " 846: 'kennedy',\n",
       " 847: 'gregg',\n",
       " 848: 'manson',\n",
       " 849: 'egotistical',\n",
       " 850: 'uni',\n",
       " 851: 'avweek',\n",
       " 852: 'loosely',\n",
       " 853: 'vest',\n",
       " 854: 'framework',\n",
       " 855: 'hillary',\n",
       " 856: 'detract',\n",
       " 857: 'slar',\n",
       " 858: 'thenet',\n",
       " 859: 'sized',\n",
       " 860: 'safe',\n",
       " 861: 'shield',\n",
       " 862: 'salt',\n",
       " 863: 'fundy',\n",
       " 864: 'hayashida',\n",
       " 865: 'philosophy',\n",
       " 866: 'spontaneously',\n",
       " 867: 'decrease',\n",
       " 868: 'vue',\n",
       " 869: 'moss',\n",
       " 870: 'archaeology',\n",
       " 871: 'exile',\n",
       " 872: 'bomb',\n",
       " 873: 'bemoan',\n",
       " 874: 'concentrate',\n",
       " 875: 'comic_strip',\n",
       " 876: 'pretention',\n",
       " 877: 'imi',\n",
       " 878: 'soc',\n",
       " 879: 'prediction',\n",
       " 880: 'waarna',\n",
       " 881: 'ground',\n",
       " 882: 'coincide',\n",
       " 883: 'headache',\n",
       " 884: 'smm',\n",
       " 885: 'polite',\n",
       " 886: 'trickle',\n",
       " 887: 'scientist',\n",
       " 888: 'boy',\n",
       " 889: 'leather',\n",
       " 890: 'trachten',\n",
       " 891: 'teh',\n",
       " 892: 'appalachian',\n",
       " 893: 'noticeably',\n",
       " 894: 'tirelessly',\n",
       " 895: 'declination',\n",
       " 896: 'legend',\n",
       " 897: 'czechoslavkia',\n",
       " 898: 'cd',\n",
       " 899: 'suspect',\n",
       " 900: 'el',\n",
       " 901: 'throwing',\n",
       " 902: 'pine',\n",
       " 903: 'sst',\n",
       " 904: 'gastronomic',\n",
       " 905: 'shackle',\n",
       " 906: 'underwriter',\n",
       " 907: 'amicable',\n",
       " 908: 'transformation',\n",
       " 909: 'permannet',\n",
       " 910: 'inthe',\n",
       " 911: 'decide',\n",
       " 912: 'fgs',\n",
       " 913: 'unum',\n",
       " 914: 'islands',\n",
       " 915: 'dense',\n",
       " 916: 'description',\n",
       " 917: 'isu',\n",
       " 918: 'intentional',\n",
       " 919: 'proxima',\n",
       " 920: 'ireland',\n",
       " 921: 'questioning',\n",
       " 922: 'distribute',\n",
       " 923: 'doubtless',\n",
       " 924: 'posy',\n",
       " 925: 'unliked',\n",
       " 926: 'werhner',\n",
       " 927: 'cont',\n",
       " 928: 'pierce',\n",
       " 929: 'gao',\n",
       " 930: 'curious',\n",
       " 931: 'glavcosmos',\n",
       " 932: 'kegs',\n",
       " 933: 'afb',\n",
       " 934: 'dogma',\n",
       " 935: 'revisionist',\n",
       " 936: 'midwesterner',\n",
       " 937: 'messiahship',\n",
       " 938: 'ham',\n",
       " 939: 'daaropvolgende',\n",
       " 940: 'baalke',\n",
       " 941: 'bizzare',\n",
       " 942: 'ispm',\n",
       " 943: 'austin',\n",
       " 944: 'program',\n",
       " 945: 'asuka',\n",
       " 946: 'rcs',\n",
       " 947: 'positions',\n",
       " 948: 'linscott',\n",
       " 949: 'dice',\n",
       " 950: 'uncorrected',\n",
       " 951: 'downey',\n",
       " 952: 'satisfactory',\n",
       " 953: 'passel',\n",
       " 954: 'andsome',\n",
       " 955: 'mysterious',\n",
       " 956: 'metalization',\n",
       " 957: 'court',\n",
       " 958: 'orville',\n",
       " 959: 'plaque',\n",
       " 960: 'wimp',\n",
       " 961: 'uh',\n",
       " 962: 'sara',\n",
       " 963: 'francisco',\n",
       " 964: 'wipp',\n",
       " 965: 'homeland',\n",
       " 966: 'disrupt',\n",
       " 967: 'casually',\n",
       " 968: 'computation',\n",
       " 969: 'oldword',\n",
       " 970: 'pizza',\n",
       " 971: 'difficulty',\n",
       " 972: 'hop',\n",
       " 973: 'purpose',\n",
       " 974: 'darwinist',\n",
       " 975: 'manipulation',\n",
       " 976: 'verify',\n",
       " 977: 'reason',\n",
       " 978: 'populum',\n",
       " 979: 'precedence',\n",
       " 980: 'priest',\n",
       " 981: 'singleton',\n",
       " 982: 'hauck',\n",
       " 983: 'vile',\n",
       " 984: 'jihad',\n",
       " 985: 'deserve',\n",
       " 986: 'tone',\n",
       " 987: 'ascii',\n",
       " 988: 'swindle',\n",
       " 989: 'foul',\n",
       " 990: 'erect',\n",
       " 991: 'score',\n",
       " 992: 'query',\n",
       " 993: 'dilute',\n",
       " 994: 'georgy',\n",
       " 995: 'perk',\n",
       " 996: 'est',\n",
       " 997: 'mariatta',\n",
       " 998: 'sheaffer',\n",
       " 999: 'extradinary',\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now turn our reviews into word vectors, and pad the text so they are all the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(clean_X, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yevgeny',\n",
       " 'gene',\n",
       " 'kilman',\n",
       " 'subject',\n",
       " 'usatoday',\n",
       " 'ad',\n",
       " 'family',\n",
       " 'value',\n",
       " 'organization',\n",
       " 'florida',\n",
       " 'international',\n",
       " 'university',\n",
       " 'miami',\n",
       " 'line',\n",
       " 'article',\n",
       " 'dan',\n",
       " 'e',\n",
       " 'babcock',\n",
       " 'write',\n",
       " 'funny',\n",
       " 'ad',\n",
       " 'usatoday',\n",
       " 'american',\n",
       " 'family',\n",
       " 'association',\n",
       " 'post',\n",
       " 'choice',\n",
       " 'part',\n",
       " 'enjoyment',\n",
       " 'emphasis',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'typo',\n",
       " 'dan',\n",
       " 'article',\n",
       " 'delete',\n",
       " 'find',\n",
       " 'add',\n",
       " 'local',\n",
       " 'sunday',\n",
       " 'newspaper',\n",
       " 'add',\n",
       " 'place',\n",
       " 'cartoon',\n",
       " 'section',\n",
       " 'perfect',\n",
       " 'place']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = len(sorted(clean_X)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_X = [[word_to_num[word] for word in text] for text in clean_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_X = sequence.pad_sequences(word_vec_X, max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,  2800, 13303,\n",
       "       11793, 11154, 11925,  5456, 14146,  4417,  6961,  5795,   784,\n",
       "        5715,  5051, 11231, 12346,  9729,  8500, 13745,  9546,  1688,\n",
       "       13202,  8214,  1645,  5568,  4860,  4995,  7732,  9256,  6091,\n",
       "        2920, 13039,  2920,  7416,  9256,  8214,  1028,   136,  9256,\n",
       "        6091,  4062,  1288,  2920, 10538,  9256,  5568,  1854,  8780,\n",
       "        9124,  8214,   185,  9256,  6091,  9206,  4001,  8780,  6091,\n",
       "        1518,  5460,   180, 10329, 13842,  2897,  3005,  2800])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(vocab_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets create the model - its a standard Sequential with an Embedding and an LSTM layer added\n",
    "\n",
    "Embedding:\n",
    "This layer takes 3 parameters - the size of the vocab (input_dims), the no. of dimensions of each word embedding (output_dim), and the length of each document (input_length), which we've standardised above. It returns a 2d matrix, with rows equal to each word in the document, and columns equal to the number of dimensions in the word embedding. \n",
    "\n",
    "*Actually its 3D, cos the batch_size is the first dimension in both input and output, but I find that confuses things more than it clarifies*\n",
    "Put another way \n",
    "\n",
    "The embedding **takes in** a factorized corpus, e.g.:\n",
    "\n",
    "**[The, cat, sat, on, the, mat]**    becomes    **[1,2,3,4,1,5]**\n",
    "\n",
    "And **outputs** a word embedded corpus:\n",
    "\n",
    "**[1,2,3,4,1,5]**    becomes (lets assume output_dim=2)   **[[0.2,0.7], [0.6,0.3], [0.1,0.8], [0.2,0.1], [0.4,0.9], [0.2,0.7]]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 125, 64)           925824    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               1181696   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,108,033\n",
      "Trainable params: 2,108,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(word_vec_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 643 samples, validate on 161 samples\n",
      "Epoch 1/2\n",
      "643/643 [==============================] - 19s 29ms/sample - loss: 1.2881 - accuracy: 0.5443 - val_loss: 0.6469 - val_accuracy: 0.7391\n",
      "Epoch 2/2\n",
      "643/643 [==============================] - 15s 24ms/sample - loss: 0.7613 - accuracy: 0.7605 - val_loss: 0.6337 - val_accuracy: 0.8509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d49ad89a08>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=2, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 216 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-a37e446549d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \"\"\"\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 152\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array 216 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_results), Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269/269 [==============================] - 3s 12ms/sample - loss: 0.6395 - accuracy: 0.8662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6394562774431307, 0.866171]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
